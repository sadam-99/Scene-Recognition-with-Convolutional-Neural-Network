{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-04T15:41:59.823501Z",
     "start_time": "2019-12-04T15:41:56.413904Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle \n",
    "import sys\n",
    "from time import *\n",
    "from cnn_model.model_loss import *\n",
    "from cnn_model.model_layers import *\n",
    "\n",
    "class CNN_NET:\n",
    "    def __init__(self):\n",
    "        # Lenet\n",
    "        # input: 100x\n",
    "        # conv1: (5x5x6)@s1p2 -> 28x28x6 {(28-5+2x2)/1+1}\n",
    "        # maxpool2: (2x2)@s2 -> 14x14x6 {(28-2)/2+1}\n",
    "        # conv3: (5x5x16)@s1p0 -> 10x10x16 {(14-5)/1+1}\n",
    "        # maxpool4: (2x2)@s2 -> 5x5x16 {(10-2)/2+1}\n",
    "        # conv5: (5x5x120)@s1p0 -> 1x1x120 {(5-5)/1+1}\n",
    "        # fc6: 120 -> 84\n",
    "        # fc7: 84 -> 10\n",
    "        # softmax: 10 -> 10\n",
    "        lr = 0.01\n",
    "        self.layers = []\n",
    "        #0\n",
    "        self.layers.append(Convolution2D(inputs_channel=3, num_filters=6, kernel_size=5, padding=2, stride=1, learning_rate=lr, name='conv1'))\n",
    "        #1\n",
    "        self.layers.append(ReLu())\n",
    "        #2\n",
    "        self.layers.append(Maxpooling2D(pool_size=2, stride=2, name='maxpool2'))\n",
    "        #3\n",
    "        self.layers.append(Convolution2D(inputs_channel=6, num_filters=16, kernel_size=5, padding=2, stride=1, learning_rate=lr, name='conv3'))\n",
    "        #4\n",
    "        self.layers.append(ReLu())\n",
    "        #5\n",
    "        self.layers.append(Maxpooling2D(pool_size=2, stride=2, name='maxpool4'))\n",
    "        #6\n",
    "        self.layers.append(Convolution2D(inputs_channel=16, num_filters=72, kernel_size=5, padding=2, stride=1, learning_rate=lr, name='conv5'))\n",
    "        #7\n",
    "        self.layers.append(ReLu())\n",
    "        #8\n",
    "        self.layers.append(Flatten())\n",
    "        #9\n",
    "        self.layers.append(FullyConnected(num_inputs=45000, num_outputs=36, learning_rate=lr, name='fc6'))\n",
    "        #10\n",
    "        self.layers.append(ReLu())\n",
    "        #11\n",
    "        self.layers.append(FullyConnected(num_inputs=36, num_outputs=6, learning_rate=lr, name='fc7'))\n",
    "        #12\n",
    "        self.layers.append(Softmax())\n",
    "        self.lay_num = len(self.layers)\n",
    "\n",
    "    def train(self, training_data, training_label, batch_size, epoch, weights_file):\n",
    "        total_acc = 0\n",
    "        for e in range(epoch):\n",
    "            for batch_index in range(0, training_data.shape[0], batch_size):\n",
    "                # batch input\n",
    "                if batch_index + batch_size < training_data.shape[0]:\n",
    "                    data = training_data[batch_index:batch_index+batch_size]\n",
    "                    label = training_label[batch_index:batch_index + batch_size]\n",
    "                else:\n",
    "                    data = training_data[batch_index:training_data.shape[0]]\n",
    "                    label = training_label[batch_index:training_label.shape[0]]\n",
    "                loss = 0\n",
    "                acc = 0\n",
    "                start_time = time()\n",
    "                for b in range(batch_size):\n",
    "                    x = data[b]\n",
    "                    y = label[b]\n",
    "                    # forward pass\n",
    "                    for l in range(self.lay_num):\n",
    "                        # print(\"Working on forward pass for layer no.\", l)\n",
    "                        output = self.layers[l].forward(x)\n",
    "                        # print(\"the shape output sfter iteration is :\",l, output.shape)\n",
    "                        x = output\n",
    "#                     print(\"output shape:\", output.shape)\n",
    "                    loss += cross_entropy(output, y)\n",
    "                    if np.argmax(output) == np.argmax(y):\n",
    "                        acc += 1\n",
    "                        total_acc += 1\n",
    "                    # print(\"output is:\", output, output.shape, np.argmax(output), np.argmax(y) )\n",
    "                    # backward pass\n",
    "                    # print(\"The Loss and accuracy is\", loss, total_acc)\n",
    "                    dy = output\n",
    "                    for l in range(self.lay_num-1, -1, -1):\n",
    "                        # print(\"Working on backward pass for layer no.\", l)\n",
    "                        dout = self.layers[l].backward(dy)\n",
    "                        dy = dout\n",
    "                # time\n",
    "                end_time = time()\n",
    "                batch_time = end_time-start_time\n",
    "                remain_time = (training_data.shape[0]*epoch-batch_index-training_data.shape[0]*e)/batch_size*batch_time\n",
    "                hrs = int(remain_time)/3600\n",
    "                mins = int((remain_time/60-hrs*60))\n",
    "                secs = int(remain_time-mins*60-hrs*3600)\n",
    "                # result\n",
    "                loss /= batch_size\n",
    "                batch_acc = float(acc)/float(batch_size)\n",
    "                training_acc = float(total_acc)/float((batch_index+batch_size)*(e+1))\n",
    "                print('=== Epoch: {0:d}/{1:d} === Iter:{2:d} === Loss: {3:.2f} === BAcc: {4:.2f} === TAcc: {5:.2f} === Remain: {6:d} Hrs {7:d} Mins {8:d} Secs ==='.format(e,epoch,batch_index+batch_size,loss,batch_acc,training_acc,int(hrs),int(mins),int(secs)))\n",
    "        # dump weights and bias\n",
    "            obj = []\n",
    "            for i in range(self.lay_num):\n",
    "                cache = self.layers[i].extract()\n",
    "                obj.append(cache)\n",
    "            with open(weights_file, 'ab') as handle:\n",
    "                pickle.dump(obj, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    def test(self, data, label, test_size):\n",
    "        # toolbar_width = 40\n",
    "        # sys.stdout.write(\"[%s]\" % (\" \" * (toolbar_width-1)))\n",
    "        # sys.stdout.flush()\n",
    "        # sys.stdout.write(\"\\b\" * (toolbar_width))\n",
    "        # step = float(test_size)/float(toolbar_width)\n",
    "        # st = 1\n",
    "        total_acc = 0\n",
    "        for i in range(test_size):\n",
    "            x = data[i]\n",
    "            y = label[i]\n",
    "            # if i == round(step):\n",
    "            #     step += float(test_size)/float(toolbar_width)\n",
    "            #     st += 1\n",
    "            #     sys.stdout.write(\".\")\n",
    "                #sys.stdout.write(\"%s]a\"%(\" \"*(toolbar_width-st)))\n",
    "                #sys.stdout.write(\"\\b\" * (toolbar_width-st+2))\n",
    "                # sys.stdout.flush()\n",
    "            \n",
    "            for l in range(self.lay_num):\n",
    "                output = self.layers[l].forward(x)\n",
    "                x = output\n",
    "            if np.argmax(output) == np.argmax(y):\n",
    "                total_acc += 1\n",
    "        sys.stdout.write(\"\\n\")\n",
    "        print('=== Test Size:{0:d} === Test Acc:{1:.2f} ==='.format(test_size, float(total_acc)/float(test_size)))\n",
    "\n",
    "    def test_with_pretrained_weights(self, data, label, test_size, weights_file):\n",
    "        with open(weights_file, 'rb') as handle:\n",
    "            b = pickle.load(handle)\n",
    "        self.layers[0].feed(b[0]['conv1.weights'], b[0]['conv1.bias'])\n",
    "        self.layers[3].feed(b[3]['conv3.weights'], b[3]['conv3.bias'])\n",
    "        self.layers[6].feed(b[6]['conv5.weights'], b[6]['conv5.bias'])\n",
    "        self.layers[9].feed(b[9]['fc6.weights'], b[9]['fc6.bias'])\n",
    "        self.layers[11].feed(b[11]['fc7.weights'], b[11]['fc7.bias'])\n",
    "        toolbar_width = 40\n",
    "        sys.stdout.write(\"[%s]\" % (\" \" * (toolbar_width-1)))\n",
    "        sys.stdout.flush()\n",
    "        sys.stdout.write(\"\\b\" * (toolbar_width))\n",
    "        step = float(test_size)/float(toolbar_width)\n",
    "        st = 1\n",
    "        total_acc = 0\n",
    "        for i in range(test_size):\n",
    "            if i == round(step):\n",
    "                step += float(test_size)/float(toolbar_width)\n",
    "                st += 1\n",
    "                sys.stdout.write(\".\")\n",
    "                #sys.stdout.write(\"%s]a\"%(\" \"*(toolbar_width-st)))\n",
    "                #sys.stdout.write(\"\\b\" * (toolbar_width-st+2))\n",
    "                sys.stdout.flush()\n",
    "            x = data[i]\n",
    "            y = label[i]\n",
    "            for l in range(self.lay_num):\n",
    "                output = self.layers[l].forward(x)\n",
    "                x = output\n",
    "            if np.argmax(output) == np.argmax(y):\n",
    "                total_acc += 1\n",
    "        sys.stdout.write(\"\\n\")\n",
    "        print('=== Test Size:{0:d} === Test Acc:{1:.2f} ==='.format(test_size, float(total_acc)/float(test_size)))\n",
    "            \n",
    "    def predict_with_pretrained_weights(self, inputs, weights_file):\n",
    "        with open(weights_file, 'rb') as handle:\n",
    "            b = pickle.load(handle)\n",
    "        self.layers[0].feed(b[0]['conv1.weights'], b[0]['conv1.bias'])\n",
    "        self.layers[3].feed(b[3]['conv3.weights'], b[3]['conv3.bias'])\n",
    "        self.layers[6].feed(b[6]['conv5.weights'], b[6]['conv5.bias'])\n",
    "        self.layers[9].feed(b[9]['fc6.weights'], b[9]['fc6.bias'])\n",
    "        self.layers[11].feed(b[11]['fc7.weights'], b[11]['fc7.bias'])\n",
    "        for l in range(self.lay_num):\n",
    "            output = self.layers[l].forward(inputs)\n",
    "            inputs = output\n",
    "        digit = np.argmax(output)\n",
    "        probability = output[0, digit]\n",
    "        return digit, probability\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-04T15:42:51.182672Z",
     "start_time": "2019-12-04T15:42:39.174706Z"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loadind and Preparing Testing Image data......\n",
      "Preparing Testing Image data Labels......\n"
     ]
    }
   ],
   "source": [
    "import glob \n",
    "import os\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as mp\n",
    "import glob\n",
    "from cnn_model.model_network import CNN_NET\n",
    "from skimage import io\n",
    "import numpy as np\n",
    "from skimage.transform import rescale, resize\n",
    "print('Loadind and Preparing Testing Image data......')\n",
    "\n",
    "# Loading the Testing Images data\n",
    "testing_img_dir = r\"test_imgs\"\n",
    "test_img = []\n",
    "for img in glob.glob( testing_img_dir + '/*' + '/*.jpg'):\n",
    "#     cv_img.append(resize(io.imread(img, as_grey = True)/255, (100, 100)))\n",
    "    test_img.append(resize(mpimg.imread(img)/255, (100, 100, 3)))\n",
    "    test_imgs = np.array(test_img)\n",
    "test_imgs -= int(np.mean(test_imgs))\n",
    "\n",
    "\n",
    "print('Preparing Testing Image data Labels......')\n",
    "\n",
    "# Creating the Testing Images Class Labels with One hot Coding\n",
    "Class_num = 6\n",
    "class_lab = []\n",
    "classes = glob.glob( testing_img_dir + '/*')\n",
    "count = 0\n",
    "for clas in classes:\n",
    "    c_len = len(glob.glob( clas + '/*.jpg'))\n",
    "    class_lab.extend([count] * c_len)\n",
    "    count += 1\n",
    "class_labels = np.array(class_lab) \n",
    "testing_labels = np.eye(Class_num)[class_labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T02:23:02.558089Z",
     "start_time": "2019-11-28T02:23:02.542088Z"
    }
   },
   "source": [
    "# Class name : Class Label\n",
    "building   :   0 \n",
    "forest     :   1\n",
    "glacier    :   2\n",
    "mountain   :   3\n",
    "sea        :   4\n",
    "street     :   5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-12-04T21:21:02.241Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing CNN......\n"
     ]
    }
   ],
   "source": [
    "test_size = 30\n",
    "CNN = CNN_NET()\n",
    "print('Testing CNN......')\n",
    "CNN.testing_trained_weights(test_imgs, testing_labels, test_size, 'cnn_model_weights.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-04T15:44:00.264462Z",
     "start_time": "2019-12-04T15:43:13.829832Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 0.16666799259951431)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_size = 30\n",
    "CNN = CNN_NET()\n",
    "CNN.predict_img_trained_weights(test_imgs[15], 'cnn_model_weights.pkl')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
